{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanmay21csu266/CSl348/blob/main/babyrobot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfwODzEXNrZg",
        "outputId": "a3786ab2-7d7f-4390-ad5b-be119ac72fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Reward Values: [0.23809524 0.25       0.52       0.70702703 0.35294118]\n",
            "Total Reward: 681\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "estimates = np.zeros(num_arms)\n",
        "arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "reward_history = []\n",
        "\n",
        "for step in range(num_steps):\n",
        "    chosen_arm = choose_arm(epsilon, estimates)\n",
        "    arm_selection_counts[chosen_arm] += 1\n",
        "    reward = np.random.binomial(1, true_rewards[chosen_arm] / 10)  # Use true_rewards as mean of a Bernoulli distribution\n",
        "    estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "    reward_history.append(reward)\n",
        "\n",
        "print(\"Estimated Reward Values:\", estimates)\n",
        "\n",
        "total_reward = np.sum(reward_history)\n",
        "print(\"Total Reward:\", total_reward)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "estimates = np.zeros(num_arms)\n",
        "arm_selection_counts = np.zeros(num_arms)\n",
        "cumulative_reward = 0\n",
        "cumulative_mean_rewards = []\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "for step in range(num_steps):\n",
        "    chosen_arm = choose_arm(epsilon, estimates)\n",
        "    arm_selection_counts[chosen_arm] += 1\n",
        "    reward = np.random.binomial(1, true_rewards[chosen_arm] / 10)  # Use true_rewards as mean of a Bernoulli distribution\n",
        "    cumulative_reward += reward\n",
        "    estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "    cumulative_mean_rewards.append(cumulative_reward / (step + 1))\n",
        "\n",
        "print(\"Cumulative Mean Rewards:\", cumulative_mean_rewards)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJ4_emooUqp8",
        "outputId": "2b6bd0ef-6bb1-425c-bda4-3962aa05a332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cumulative Mean Rewards: [1.0, 1.0, 1.0, 0.75, 0.6, 0.6666666666666666, 0.5714285714285714, 0.5, 0.4444444444444444, 0.4, 0.45454545454545453, 0.4166666666666667, 0.38461538461538464, 0.42857142857142855, 0.4666666666666667, 0.4375, 0.4117647058823529, 0.3888888888888889, 0.3684210526315789, 0.4, 0.42857142857142855, 0.4090909090909091, 0.391304347826087, 0.375, 0.36, 0.38461538461538464, 0.4074074074074074, 0.42857142857142855, 0.41379310344827586, 0.4, 0.3870967741935484, 0.375, 0.3939393939393939, 0.4117647058823529, 0.4, 0.3888888888888889, 0.3783783783783784, 0.3684210526315789, 0.358974358974359, 0.35, 0.34146341463414637, 0.35714285714285715, 0.37209302325581395, 0.38636363636363635, 0.37777777777777777, 0.391304347826087, 0.40425531914893614, 0.3958333333333333, 0.3877551020408163, 0.38, 0.39215686274509803, 0.40384615384615385, 0.39622641509433965, 0.3888888888888889, 0.4, 0.39285714285714285, 0.40350877192982454, 0.39655172413793105, 0.4067796610169492, 0.4, 0.39344262295081966, 0.4032258064516129, 0.3968253968253968, 0.390625, 0.38461538461538464, 0.3939393939393939, 0.40298507462686567, 0.39705882352941174, 0.391304347826087, 0.38571428571428573, 0.39436619718309857, 0.3888888888888889, 0.3835616438356164, 0.3783783783783784, 0.37333333333333335, 0.3684210526315789, 0.37662337662337664, 0.38461538461538464, 0.3924050632911392, 0.3875, 0.3950617283950617, 0.4024390243902439, 0.40963855421686746, 0.4166666666666667, 0.4235294117647059, 0.43023255813953487, 0.4367816091954023, 0.4318181818181818, 0.43820224719101125, 0.43333333333333335, 0.42857142857142855, 0.43478260869565216, 0.43010752688172044, 0.43617021276595747, 0.43157894736842106, 0.4270833333333333, 0.4329896907216495, 0.4387755102040816, 0.4444444444444444, 0.45, 0.44554455445544555, 0.4411764705882353, 0.44660194174757284, 0.4519230769230769, 0.45714285714285713, 0.46226415094339623, 0.4672897196261682, 0.4722222222222222, 0.47706422018348627, 0.4818181818181818, 0.4864864864864865, 0.49107142857142855, 0.49557522123893805, 0.5, 0.4956521739130435, 0.5, 0.5042735042735043, 0.5, 0.5042016806722689, 0.5083333333333333, 0.512396694214876, 0.5081967213114754, 0.5121951219512195, 0.5161290322580645, 0.52, 0.5238095238095238, 0.5275590551181102, 0.5234375, 0.5193798449612403, 0.5153846153846153, 0.5114503816793893, 0.5151515151515151, 0.5112781954887218, 0.5149253731343284, 0.5185185185185185, 0.5147058823529411, 0.5182481751824818, 0.5217391304347826, 0.5251798561151079, 0.5285714285714286, 0.524822695035461, 0.528169014084507, 0.5244755244755245, 0.5277777777777778, 0.5310344827586206, 0.5342465753424658, 0.5306122448979592, 0.5337837837837838, 0.5302013422818792, 0.5333333333333333, 0.5364238410596026, 0.5394736842105263, 0.5424836601307189, 0.5454545454545454, 0.5419354838709678, 0.5448717948717948, 0.5414012738853503, 0.5443037974683544, 0.5471698113207547, 0.54375, 0.5403726708074534, 0.5370370370370371, 0.5398773006134969, 0.5365853658536586, 0.5393939393939394, 0.536144578313253, 0.5329341317365269, 0.5357142857142857, 0.5325443786982249, 0.5352941176470588, 0.5321637426900585, 0.5290697674418605, 0.5317919075144508, 0.5344827586206896, 0.5314285714285715, 0.5284090909090909, 0.5254237288135594, 0.5280898876404494, 0.5307262569832403, 0.5333333333333333, 0.5359116022099447, 0.5384615384615384, 0.5409836065573771, 0.5380434782608695, 0.5351351351351351, 0.532258064516129, 0.5294117647058824, 0.526595744680851, 0.5238095238095238, 0.5263157894736842, 0.5287958115183246, 0.53125, 0.533678756476684, 0.5360824742268041, 0.5384615384615384, 0.5357142857142857, 0.5380710659898477, 0.5404040404040404, 0.542713567839196, 0.54, 0.5373134328358209, 0.5396039603960396, 0.541871921182266, 0.5441176470588235, 0.5414634146341464, 0.5436893203883495, 0.5410628019323671, 0.5432692307692307, 0.5454545454545454, 0.5428571428571428, 0.5450236966824644, 0.5471698113207547, 0.5446009389671361, 0.5420560747663551, 0.5441860465116279, 0.5416666666666666, 0.543778801843318, 0.5412844036697247, 0.54337899543379, 0.5454545454545454, 0.5475113122171946, 0.5495495495495496, 0.547085201793722, 0.5491071428571429, 0.5511111111111111, 0.5530973451327433, 0.5506607929515418, 0.5526315789473685, 0.5502183406113537, 0.5478260869565217, 0.5497835497835498, 0.5517241379310345, 0.5536480686695279, 0.5555555555555556, 0.5574468085106383, 0.559322033898305, 0.5611814345991561, 0.5588235294117647, 0.5564853556485355, 0.5583333333333333, 0.5560165975103735, 0.5537190082644629, 0.5555555555555556, 0.5532786885245902, 0.5510204081632653, 0.5528455284552846, 0.5546558704453441, 0.5564516129032258, 0.5582329317269076, 0.56, 0.5577689243027888, 0.5595238095238095, 0.5612648221343873, 0.562992125984252, 0.5647058823529412, 0.56640625, 0.5680933852140078, 0.5697674418604651, 0.5675675675675675, 0.5692307692307692, 0.5670498084291188, 0.5648854961832062, 0.5665399239543726, 0.5681818181818182, 0.569811320754717, 0.5714285714285714, 0.5692883895131086, 0.5708955223880597, 0.5724907063197026, 0.5740740740740741, 0.5756457564575646, 0.5772058823529411, 0.5787545787545788, 0.5802919708029197, 0.5818181818181818, 0.5833333333333334, 0.5848375451263538, 0.5827338129496403, 0.5806451612903226, 0.5785714285714286, 0.5800711743772242, 0.5780141843971631, 0.5795053003533569, 0.5774647887323944, 0.5789473684210527, 0.5804195804195804, 0.5818815331010453, 0.5833333333333334, 0.5847750865051903, 0.5827586206896552, 0.584192439862543, 0.5856164383561644, 0.5870307167235495, 0.5884353741496599, 0.5898305084745763, 0.5912162162162162, 0.5925925925925926, 0.5939597315436241, 0.5953177257525084, 0.5966666666666667, 0.5980066445182725, 0.5993377483443708, 0.6006600660066007, 0.5986842105263158, 0.5967213114754099, 0.5947712418300654, 0.5960912052117264, 0.5941558441558441, 0.5922330097087378, 0.5935483870967742, 0.594855305466238, 0.5961538461538461, 0.597444089456869, 0.5955414012738853, 0.5968253968253968, 0.5981012658227848, 0.5993690851735016, 0.5974842767295597, 0.5987460815047022, 0.6, 0.5981308411214953, 0.5993788819875776, 0.6006191950464397, 0.6018518518518519, 0.6030769230769231, 0.6012269938650306, 0.6024464831804281, 0.6036585365853658, 0.601823708206687, 0.603030303030303, 0.6042296072507553, 0.6054216867469879, 0.6066066066066066, 0.6047904191616766, 0.6059701492537314, 0.6071428571428571, 0.6053412462908012, 0.606508875739645, 0.6076696165191741, 0.6088235294117647, 0.6099706744868035, 0.6111111111111112, 0.6122448979591837, 0.6133720930232558, 0.6144927536231884, 0.6127167630057804, 0.6109510086455331, 0.6120689655172413, 0.6131805157593123, 0.6114285714285714, 0.6125356125356125, 0.6107954545454546, 0.6090651558073654, 0.6073446327683616, 0.6084507042253521, 0.6067415730337079, 0.6050420168067226, 0.6033519553072626, 0.6016713091922006, 0.6027777777777777, 0.6038781163434903, 0.6022099447513812, 0.6033057851239669, 0.6043956043956044, 0.6027397260273972, 0.6038251366120219, 0.6049046321525886, 0.6032608695652174, 0.6043360433604336, 0.6027027027027027, 0.6037735849056604, 0.6048387096774194, 0.6032171581769437, 0.6042780748663101, 0.6053333333333333, 0.6063829787234043, 0.6074270557029178, 0.6084656084656085, 0.6094986807387863, 0.6105263157894737, 0.6089238845144357, 0.6099476439790575, 0.6109660574412533, 0.6119791666666666, 0.612987012987013, 0.6139896373056994, 0.6124031007751938, 0.6108247422680413, 0.6118251928020566, 0.6102564102564103, 0.6112531969309463, 0.6122448979591837, 0.6132315521628499, 0.6142131979695431, 0.6151898734177215, 0.6136363636363636, 0.6146095717884131, 0.6155778894472361, 0.6165413533834586, 0.615, 0.6159600997506235, 0.6169154228855721, 0.6178660049627791, 0.6163366336633663, 0.6172839506172839, 0.6182266009852216, 0.6191646191646192, 0.6200980392156863, 0.6185819070904646, 0.6195121951219512, 0.6204379562043796, 0.6189320388349514, 0.6198547215496368, 0.6207729468599034, 0.6216867469879518, 0.6225961538461539, 0.6211031175059952, 0.6220095693779905, 0.6229116945107399, 0.6214285714285714, 0.6223277909738717, 0.6208530805687204, 0.6193853427895981, 0.6202830188679245, 0.6211764705882353, 0.6220657276995305, 0.6229508196721312, 0.6238317757009346, 0.6223776223776224, 0.6209302325581395, 0.6218097447795824, 0.6203703703703703, 0.6212471131639723, 0.619815668202765, 0.6206896551724138, 0.6192660550458715, 0.620137299771167, 0.6210045662100456, 0.621867881548975, 0.6227272727272727, 0.6235827664399093, 0.6244343891402715, 0.6230248306997742, 0.6238738738738738, 0.6247191011235955, 0.625560538116592, 0.6241610738255033, 0.625, 0.6258351893095768, 0.6244444444444445, 0.623059866962306, 0.6216814159292036, 0.6225165562913907, 0.6233480176211453, 0.6241758241758242, 0.625, 0.6258205689277899, 0.6266375545851528, 0.6252723311546841, 0.6239130434782608, 0.6247288503253796, 0.6255411255411255, 0.6241900647948164, 0.6228448275862069, 0.6236559139784946, 0.6223175965665236, 0.6209850107066381, 0.6196581196581197, 0.6183368869936035, 0.6170212765957447, 0.6178343949044586, 0.6186440677966102, 0.6194503171247357, 0.620253164556962, 0.6210526315789474, 0.6218487394957983, 0.6226415094339622, 0.6234309623430963, 0.6242171189979123, 0.625, 0.6257796257796258, 0.6265560165975104, 0.6273291925465838, 0.628099173553719, 0.6288659793814433, 0.6296296296296297, 0.6303901437371663, 0.6290983606557377, 0.6298568507157464, 0.6285714285714286, 0.6293279022403259, 0.6300813008130082, 0.6308316430020284, 0.6295546558704453, 0.6303030303030303, 0.6310483870967742, 0.6297786720321932, 0.6305220883534136, 0.6312625250501002, 0.632, 0.6327345309381237, 0.6334661354581673, 0.6341948310139165, 0.6349206349206349, 0.6356435643564357, 0.6363636363636364, 0.6370808678500987, 0.6358267716535433, 0.6345776031434185, 0.6333333333333333, 0.6320939334637965, 0.630859375, 0.631578947368421, 0.6303501945525292, 0.629126213592233, 0.6298449612403101, 0.6305609284332688, 0.6312741312741312, 0.6319845857418112, 0.6326923076923077, 0.6333973128598849, 0.6340996168582376, 0.6347992351816444, 0.6354961832061069, 0.6361904761904762, 0.6368821292775665, 0.6375711574952562, 0.6382575757575758, 0.6370510396975425, 0.6358490566037736, 0.6365348399246704, 0.6353383458646616, 0.6341463414634146, 0.6329588014981273, 0.6317757009345795, 0.6324626865671642, 0.6312849162011173, 0.6319702602230484, 0.6307977736549165, 0.6314814814814815, 0.6321626617375231, 0.6309963099630996, 0.6316758747697975, 0.6323529411764706, 0.6311926605504588, 0.6318681318681318, 0.6325411334552102, 0.6332116788321168, 0.6338797814207651, 0.6345454545454545, 0.6352087114337568, 0.6358695652173914, 0.6347197106690777, 0.6353790613718412, 0.6360360360360361, 0.6366906474820144, 0.6373429084380611, 0.6362007168458781, 0.6368515205724508, 0.6357142857142857, 0.6363636363636364, 0.6352313167259787, 0.6358792184724689, 0.6347517730496454, 0.6336283185840708, 0.6342756183745583, 0.6349206349206349, 0.6355633802816901, 0.6362038664323374, 0.6350877192982456, 0.6339754816112084, 0.6328671328671329, 0.6335078534031413, 0.6341463414634146, 0.6347826086956522, 0.6354166666666666, 0.634315424610052, 0.6332179930795848, 0.6338514680483592, 0.6344827586206897, 0.6351118760757315, 0.634020618556701, 0.6346483704974271, 0.6352739726027398, 0.6358974358974359, 0.636518771331058, 0.637137989778535, 0.6360544217687075, 0.634974533106961, 0.6338983050847458, 0.6345177664974619, 0.6334459459459459, 0.6323777403035413, 0.632996632996633, 0.6336134453781512, 0.6342281879194631, 0.6348408710217756, 0.6337792642140468, 0.6343906510851419, 0.635, 0.6356073211314476, 0.6362126245847176, 0.6368159203980099, 0.6374172185430463, 0.6380165289256199, 0.6386138613861386, 0.6392092257001647, 0.6381578947368421, 0.638752052545156, 0.639344262295082, 0.6382978723404256, 0.6388888888888888, 0.6394779771615008, 0.6384364820846905, 0.6373983739837399, 0.637987012987013, 0.6385737439222042, 0.63915857605178, 0.6381260096930533, 0.6387096774193548, 0.6392914653784219, 0.639871382636656, 0.6404494382022472, 0.6394230769230769, 0.64, 0.6389776357827476, 0.6379585326953748, 0.6385350318471338, 0.6391096979332274, 0.638095238095238, 0.6386687797147385, 0.6392405063291139, 0.6398104265402843, 0.638801261829653, 0.6393700787401575, 0.6383647798742138, 0.6389324960753532, 0.6379310344827587, 0.6384976525821596, 0.6390625, 0.6380655226209049, 0.6370716510903427, 0.6376360808709176, 0.6381987577639752, 0.6387596899224807, 0.6377708978328174, 0.6383307573415765, 0.6388888888888888, 0.6394453004622496, 0.64, 0.6405529953917051, 0.6411042944785276, 0.6401225114854517, 0.6391437308868502, 0.6381679389312978, 0.6387195121951219, 0.639269406392694, 0.6398176291793313, 0.6403641881638846, 0.6409090909090909, 0.6414523449319214, 0.6419939577039275, 0.6425339366515838, 0.6430722891566265, 0.643609022556391, 0.6441441441441441, 0.6446776611694153, 0.6452095808383234, 0.6457399103139013, 0.6462686567164179, 0.646795827123696, 0.6473214285714286, 0.6463595839524517, 0.6468842729970327, 0.6474074074074074, 0.6479289940828402, 0.6469719350073855, 0.6474926253687315, 0.6480117820324006, 0.6485294117647059, 0.6490455212922174, 0.6495601173020528, 0.6500732064421669, 0.6505847953216374, 0.6496350364963503, 0.6501457725947521, 0.6506550218340611, 0.6511627906976745, 0.6502177068214804, 0.6492753623188405, 0.6497829232995659, 0.6502890173410405, 0.6507936507936508, 0.6512968299711815, 0.6503597122302158, 0.6508620689655172, 0.6513629842180775, 0.6518624641833811, 0.6523605150214592, 0.6514285714285715, 0.6504992867332382, 0.6495726495726496, 0.6500711237553343, 0.6505681818181818, 0.6510638297872341, 0.6515580736543909, 0.652050919377652, 0.652542372881356, 0.6530324400564175, 0.652112676056338, 0.6526019690576652, 0.6530898876404494, 0.6521739130434783, 0.6526610644257703, 0.6531468531468532, 0.6522346368715084, 0.6513249651324965, 0.6518105849582173, 0.650904033379694, 0.6513888888888889, 0.6504854368932039, 0.6509695290858726, 0.6500691562932227, 0.649171270718232, 0.6482758620689655, 0.6487603305785123, 0.6492434662998624, 0.6497252747252747, 0.6502057613168725, 0.6493150684931507, 0.6497948016415869, 0.6489071038251366, 0.6493860845839018, 0.6498637602179836, 0.6503401360544218, 0.6508152173913043, 0.6512890094979648, 0.6504065040650406, 0.6508795669824087, 0.6513513513513514, 0.6518218623481782, 0.6509433962264151, 0.6500672947510094, 0.6491935483870968, 0.6496644295302013, 0.6501340482573726, 0.6506024096385542, 0.6497326203208557, 0.650200267022697, 0.6493333333333333, 0.6498002663115846, 0.648936170212766, 0.649402390438247, 0.649867374005305, 0.6490066225165563, 0.6481481481481481, 0.6486129458388376, 0.6490765171503958, 0.6495388669301713, 0.65, 0.6504599211563732, 0.6509186351706037, 0.6513761467889908, 0.6518324607329843, 0.6522875816993464, 0.6527415143603134, 0.651890482398957, 0.65234375, 0.6527958387516255, 0.6532467532467533, 0.6523994811932555, 0.6528497409326425, 0.6520051746442432, 0.6524547803617571, 0.6529032258064517, 0.6533505154639175, 0.6537966537966537, 0.6529562982005142, 0.6521181001283697, 0.6525641025641026, 0.6530089628681178, 0.6534526854219949, 0.6526181353767561, 0.6530612244897959, 0.6535031847133758, 0.6539440203562341, 0.6531130876747141, 0.6535532994923858, 0.6539923954372624, 0.6544303797468355, 0.6548672566371682, 0.6540404040404041, 0.6544766708701135, 0.654911838790932, 0.6540880503144654, 0.6545226130653267, 0.6537013801756587, 0.6541353383458647, 0.6545682102628285, 0.655, 0.6554307116104869, 0.655860349127182, 0.6562889165628891, 0.6554726368159204, 0.6559006211180124, 0.6563275434243176, 0.6567534076827757, 0.655940594059406, 0.6563658838071693, 0.6567901234567901, 0.657213316892725, 0.6576354679802956, 0.6568265682656826, 0.6572481572481572, 0.6576687116564417, 0.6580882352941176, 0.6585067319461444, 0.6577017114914425, 0.6581196581196581, 0.6585365853658537, 0.658952496954933, 0.6593673965936739, 0.6585662211421628, 0.6589805825242718, 0.6593939393939394, 0.6585956416464891, 0.6577992744860943, 0.6570048309178744, 0.6562123039806996, 0.6566265060240963, 0.6570397111913358, 0.6574519230769231, 0.6566626650660264, 0.657074340527578, 0.6574850299401198, 0.6578947368421053, 0.6571087216248507, 0.6575178997613366, 0.6567342073897497, 0.6571428571428571, 0.6575505350772889, 0.6567695961995249, 0.6571767497034401, 0.6575829383886256, 0.6579881656804734, 0.6583924349881797, 0.6587957497048406, 0.6591981132075472, 0.6595995288574794, 0.6588235294117647, 0.6592244418331374, 0.6596244131455399, 0.6588511137162955, 0.65807962529274, 0.6584795321637427, 0.6577102803738317, 0.6581096849474912, 0.6585081585081585, 0.6577415599534342, 0.6569767441860465, 0.6573751451800233, 0.6577726218097448, 0.657010428736964, 0.6574074074074074, 0.6578034682080924, 0.6581986143187067, 0.657439446366782, 0.6578341013824884, 0.6582278481012658, 0.6586206896551724, 0.6578645235361653, 0.6571100917431193, 0.6575028636884307, 0.6578947368421053, 0.6571428571428571, 0.656392694063927, 0.6567844925883695, 0.6571753986332574, 0.6575654152445961, 0.6568181818181819, 0.6572077185017026, 0.6575963718820862, 0.6579841449603624, 0.6572398190045249, 0.656497175141243, 0.6568848758465011, 0.657271702367531, 0.6576576576576577, 0.6569178852643419, 0.6561797752808989, 0.6554433221099888, 0.6547085201793722, 0.6550951847704367, 0.6554809843400448, 0.6558659217877095, 0.65625, 0.6566332218506131, 0.6570155902004454, 0.657397107897664, 0.6566666666666666, 0.6570477247502775, 0.656319290465632, 0.6566998892580288, 0.6570796460176991, 0.6574585635359116, 0.6578366445916115, 0.6582138919514884, 0.6585903083700441, 0.6578657865786579, 0.6582417582417582, 0.6575192096597146, 0.6567982456140351, 0.6571741511500547, 0.6564551422319475, 0.6568306010928961, 0.6572052401746725, 0.6564885496183206, 0.6568627450980392, 0.6572361262241567, 0.657608695652174, 0.6568946796959826, 0.6572668112798264, 0.6576381365113759, 0.658008658008658, 0.6583783783783784, 0.6587473002159827, 0.6591154261057174, 0.6584051724137931, 0.6576964477933261, 0.6580645161290323, 0.6584317937701396, 0.657725321888412, 0.6580921757770632, 0.6573875802997858, 0.6577540106951871, 0.6581196581196581, 0.6584845250800427, 0.6588486140724946, 0.65814696485623, 0.6574468085106383, 0.6567481402763018, 0.6571125265392781, 0.6574761399787911, 0.6567796610169492, 0.6571428571428571, 0.6575052854122622, 0.6568109820485745, 0.6571729957805907, 0.6575342465753424, 0.6568421052631579, 0.6561514195583596, 0.6565126050420168, 0.6558237145855194, 0.6561844863731656, 0.6565445026178011, 0.6569037656903766, 0.6572622779519331, 0.6576200417536534, 0.6579770594369134, 0.6572916666666667, 0.6576482830385015, 0.656964656964657, 0.6573208722741433, 0.6576763485477178, 0.6569948186528497, 0.6573498964803313, 0.656670113753878, 0.65599173553719, 0.6553147574819401, 0.6556701030927835, 0.6560247167868177, 0.6553497942386831, 0.6557040082219938, 0.6550308008213552, 0.6553846153846153, 0.6557377049180327, 0.6560900716479018, 0.6554192229038854, 0.6557711950970377, 0.6561224489795918, 0.6564729867482161, 0.6568228105906314, 0.6571719226856562, 0.657520325203252, 0.6568527918781726, 0.6572008113590264, 0.6575481256332321, 0.6578947368421053, 0.6582406471183013, 0.6575757575757576, 0.6579212916246215, 0.6582661290322581, 0.6586102719033232, 0.6589537223340041, 0.6592964824120603, 0.6596385542168675, 0.6589769307923772, 0.6593186372745491, 0.6596596596596597, 0.659]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [0.4, 0.3, 0.5, 0.7, 0.2]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "total_rewards_per_trial = []\n",
        "cumulative_mean_rewards_per_trial = []\n",
        "\n",
        "def choose_arm(epsilon, estimates):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.choice(num_arms)\n",
        "    else:\n",
        "        return np.argmax(estimates)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "    cumulative_mean_rewards = []\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        chosen_arm = choose_arm(epsilon, estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "        cumulative_mean_rewards.append(cumulative_reward / (step + 1))\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    cumulative_mean_rewards_per_trial.append(cumulative_mean_rewards)\n",
        "\n",
        "average_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "# Calculate the average cumulative mean rewards after all trials\n",
        "average_cumulative_mean_rewards = np.mean(cumulative_mean_rewards_per_trial, axis=0)\n",
        "\n",
        "print(\"Average Total Reward across Trials:\", average_total_reward)\n",
        "print(\"Average Cumulative Mean Rewards across Trials:\", average_cumulative_mean_rewards)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upKk8nb9VRI_",
        "outputId": "75cdea6f-a4be-4cf9-bfb4-73dcdf5c0b4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Total Reward across Trials: 420.214\n",
            "Average Cumulative Mean Rewards across Trials: [0.405      0.4225     0.42633333 0.42925    0.4258     0.426\n",
            " 0.41942857 0.417375   0.41677778 0.4167     0.41909091 0.41791667\n",
            " 0.41953846 0.41921429 0.4188     0.419375   0.42094118 0.42005556\n",
            " 0.42021053 0.41955    0.41880952 0.41922727 0.42034783 0.421125\n",
            " 0.42068    0.42092308 0.42048148 0.42046429 0.421      0.4206\n",
            " 0.42090323 0.421125   0.42015152 0.41994118 0.41911429 0.41872222\n",
            " 0.41864865 0.41947368 0.41992308 0.420075   0.41956098 0.41878571\n",
            " 0.41860465 0.41856818 0.41873333 0.41902174 0.41861702 0.4183125\n",
            " 0.41859184 0.41794    0.41817647 0.41828846 0.41826415 0.41807407\n",
            " 0.41801818 0.41846429 0.41849123 0.41925862 0.41881356 0.41915\n",
            " 0.41921311 0.41930645 0.41965079 0.4196875  0.42015385 0.4205\n",
            " 0.42025373 0.42023529 0.42036232 0.4204     0.42046479 0.42019444\n",
            " 0.4199589  0.42012162 0.42038667 0.42039474 0.42045455 0.42025641\n",
            " 0.42032911 0.4203125  0.42023457 0.42031707 0.42045783 0.42059524\n",
            " 0.42049412 0.42031395 0.42028736 0.42039773 0.42002247 0.42035556\n",
            " 0.42043956 0.42034783 0.4205914  0.42069149 0.42108421 0.42127083\n",
            " 0.42143299 0.42142857 0.42159596 0.42188    0.42219802 0.42232353\n",
            " 0.42206796 0.42201923 0.42215238 0.42183019 0.42174766 0.42162963\n",
            " 0.42158716 0.42177273 0.42167568 0.42163393 0.42151327 0.42157895\n",
            " 0.42150435 0.42123276 0.42132479 0.42131356 0.42110924 0.42120833\n",
            " 0.42117355 0.42103279 0.4211626  0.42127419 0.421352   0.42122222\n",
            " 0.42127559 0.42133594 0.42129457 0.42122308 0.42110687 0.42121212\n",
            " 0.42137594 0.42144776 0.42146667 0.42154412 0.42160584 0.42168116\n",
            " 0.42171942 0.42169286 0.42177305 0.42180282 0.42169231 0.42193056\n",
            " 0.42204138 0.42213014 0.42203401 0.422      0.42183893 0.42189333\n",
            " 0.42187417 0.42176316 0.42179739 0.42174675 0.4217871  0.42191667\n",
            " 0.42194268 0.42198101 0.42202516 0.42199375 0.42201863 0.42207407\n",
            " 0.42196319 0.42195732 0.42186061 0.42180723 0.42191018 0.4218869\n",
            " 0.42173964 0.42170588 0.42188889 0.42198256 0.42178613 0.4216954\n",
            " 0.42168571 0.42164773 0.42164972 0.42168539 0.42160894 0.42153889\n",
            " 0.42145304 0.42159341 0.42146995 0.42150543 0.42156216 0.42168817\n",
            " 0.42174332 0.4217766  0.42152381 0.42138421 0.42157592 0.4215\n",
            " 0.42165803 0.42153093 0.42157436 0.4214898  0.42145178 0.42155556\n",
            " 0.42155779 0.42146    0.42138308 0.4215396  0.42161084 0.42153922\n",
            " 0.42142927 0.42135437 0.42151208 0.42166346 0.42187081 0.42177619\n",
            " 0.42170616 0.42167925 0.42170892 0.42186449 0.42193488 0.42191667\n",
            " 0.42184332 0.42177523 0.42184932 0.42186364 0.42175113 0.42171171\n",
            " 0.42167265 0.42177232 0.42183556 0.42178761 0.42179295 0.42174123\n",
            " 0.42179039 0.42182174 0.42190909 0.42209052 0.42209442 0.42215812\n",
            " 0.42211915 0.42210593 0.42216034 0.42223109 0.42223431 0.42219167\n",
            " 0.42220747 0.42217355 0.42231687 0.42222131 0.42212653 0.42214228\n",
            " 0.42209717 0.42220161 0.42217269 0.42208    0.42194422 0.42201587\n",
            " 0.42200791 0.422      0.42202745 0.42196484 0.42191051 0.42186047\n",
            " 0.42189575 0.42198077 0.42195019 0.4219771  0.42203802 0.42206439\n",
            " 0.42209057 0.42215038 0.42222472 0.42214925 0.42213383 0.42212963\n",
            " 0.42215129 0.42222059 0.42220879 0.42230292 0.42232    0.42236232\n",
            " 0.42238267 0.42242446 0.42227957 0.42220714 0.42217794 0.42206383\n",
            " 0.42209187 0.42214789 0.42222105 0.42212587 0.42212544 0.42211806\n",
            " 0.42210727 0.4220931  0.42206873 0.42197945 0.42192491 0.42183673\n",
            " 0.42182373 0.42186149 0.42187205 0.42181879 0.4217893  0.42171667\n",
            " 0.42175748 0.42182119 0.42186139 0.42188158 0.42182623 0.42185621\n",
            " 0.42192182 0.42183442 0.42185761 0.42178387 0.42186817 0.4218141\n",
            " 0.42185304 0.42183758 0.42177143 0.42172468 0.42180442 0.42178616\n",
            " 0.42177429 0.4217375  0.4217757  0.42173292 0.4217678  0.42179938\n",
            " 0.42179385 0.42183436 0.42176758 0.42169817 0.42175684 0.42179394\n",
            " 0.42182477 0.42180422 0.42176877 0.42175749 0.42175821 0.42172619\n",
            " 0.42175074 0.42173669 0.42181121 0.42181471 0.42177126 0.42177778\n",
            " 0.42178426 0.42173256 0.42173623 0.4217341  0.42169741 0.42166092\n",
            " 0.42165903 0.42171143 0.42167806 0.42171591 0.42170822 0.42169209\n",
            " 0.42166761 0.42162079 0.42157143 0.42148883 0.42151253 0.42147778\n",
            " 0.42152909 0.4215221  0.42149311 0.42145604 0.42141096 0.42144536\n",
            " 0.42144414 0.42137772 0.42130894 0.42135946 0.42132075 0.42129839\n",
            " 0.42132708 0.42132086 0.421264   0.42121809 0.42128647 0.42127513\n",
            " 0.42120317 0.42119737 0.42119948 0.42127749 0.42127415 0.42125521\n",
            " 0.42124156 0.42121503 0.42122997 0.4212268  0.42120051 0.42112821\n",
            " 0.42114834 0.42114796 0.42111705 0.42108122 0.42107595 0.42107828\n",
            " 0.42107053 0.4210603  0.42100251 0.42098    0.42100998 0.42101244\n",
            " 0.42100744 0.4210198  0.42097531 0.42090394 0.42089681 0.4208799\n",
            " 0.42088264 0.42083171 0.42086861 0.42078398 0.42080387 0.42081643\n",
            " 0.42082651 0.42077885 0.42081055 0.42079187 0.42077327 0.42079048\n",
            " 0.42081473 0.42081043 0.42078723 0.42077358 0.42085176 0.42088028\n",
            " 0.42086183 0.42086916 0.42079254 0.42073953 0.42075174 0.42077315\n",
            " 0.42080831 0.42076728 0.42078851 0.42080505 0.42078947 0.4207968\n",
            " 0.42077449 0.42075909 0.42077778 0.42076923 0.42077878 0.42072748\n",
            " 0.42075056 0.420787   0.42072931 0.42079018 0.42084187 0.42082222\n",
            " 0.42080931 0.42081416 0.42083664 0.42081498 0.42079121 0.42077851\n",
            " 0.42075274 0.42074891 0.42072985 0.42075435 0.4208026  0.42077489\n",
            " 0.42077322 0.42076293 0.42075269 0.42074249 0.42068522 0.4207265\n",
            " 0.42069723 0.42071064 0.42074522 0.42071822 0.42069979 0.42068143\n",
            " 0.42065895 0.42058613 0.42054507 0.4205251  0.42048852 0.42050208\n",
            " 0.42052807 0.4205     0.42044928 0.42047107 0.42050928 0.42051029\n",
            " 0.4204846  0.42045902 0.42047239 0.42043469 0.42044603 0.4204878\n",
            " 0.42043611 0.42040486 0.42036768 0.42036895 0.42036217 0.42040161\n",
            " 0.42035872 0.420382   0.42032335 0.42034462 0.42039364 0.42039286\n",
            " 0.42035842 0.42037945 0.42036292 0.4203937  0.42041257 0.4204\n",
            " 0.42038356 0.42039063 0.4204425  0.42049416 0.42048155 0.42047287\n",
            " 0.42044487 0.42041506 0.4204316  0.42045962 0.42043762 0.42042529\n",
            " 0.42043021 0.42043511 0.42046476 0.42043916 0.42045541 0.42049811\n",
            " 0.42044612 0.42041321 0.4204275  0.42046053 0.42045966 0.42043071\n",
            " 0.42043364 0.42040858 0.42042644 0.42038848 0.42040074 0.42039815\n",
            " 0.42037893 0.42038561 0.42038858 0.42039338 0.42044404 0.4204011\n",
            " 0.42038574 0.42038321 0.42033698 0.42030727 0.42032123 0.42031522\n",
            " 0.42033816 0.42035199 0.42032072 0.42030036 0.4202693  0.42025986\n",
            " 0.42026118 0.42026429 0.42024064 0.42026512 0.42027531 0.4202961\n",
            " 0.42027611 0.42026678 0.42025926 0.4202007  0.42019156 0.42021053\n",
            " 0.42018389 0.42015734 0.42012216 0.42012021 0.42010261 0.42006944\n",
            " 0.42007972 0.42006401 0.42008808 0.42016207 0.42013253 0.42014777\n",
            " 0.42010635 0.42007363 0.42009573 0.4201058  0.42009199 0.42010884\n",
            " 0.42012394 0.42010508 0.42009306 0.42009966 0.42005396 0.42007912\n",
            " 0.42007899 0.42009564 0.42010218 0.42012375 0.4200985  0.42015667\n",
            " 0.42016306 0.42017608 0.42014262 0.42008609 0.4200876  0.42008581\n",
            " 0.42009885 0.42011513 0.42008539 0.4200918  0.4200491  0.42006373\n",
            " 0.42009788 0.42012866 0.42014146 0.42015584 0.42012804 0.42015858\n",
            " 0.42017447 0.42016935 0.42013043 0.42012058 0.42014928 0.42011699\n",
            " 0.42016    0.42015815 0.42014833 0.4201465  0.42017329 0.42017619\n",
            " 0.42015848 0.42015032 0.42011848 0.42014511 0.42014961 0.42011006\n",
            " 0.42008163 0.42010188 0.42015493 0.42013906 0.42018877 0.42018536\n",
            " 0.42015397 0.42011801 0.42009922 0.42014396 0.42013138 0.42012346\n",
            " 0.42009091 0.42006923 0.42006144 0.42008742 0.42006738 0.4200474\n",
            " 0.42001527 0.42004116 0.42007458 0.42007447 0.42006677 0.42009091\n",
            " 0.42007413 0.42007704 0.42008296 0.42004819 0.42003759 0.42007357\n",
            " 0.42005247 0.42006886 0.42008371 0.42009403 0.4200924  0.42009673\n",
            " 0.4200847  0.42007418 0.42005185 0.42006065 0.42008567 0.42007965\n",
            " 0.42005007 0.42006912 0.42007342 0.42008798 0.42012152 0.42013012\n",
            " 0.42018248 0.42016181 0.42018777 0.42017587 0.42014369 0.42013623\n",
            " 0.42014761 0.42016329 0.42015296 0.42023487 0.42024317 0.42023851\n",
            " 0.42023386 0.4202149  0.42022747 0.42020857 0.42022111 0.42026068\n",
            " 0.42024751 0.42025852 0.42025816 0.42024646 0.42027723 0.42028814\n",
            " 0.42026657 0.42024366 0.42024332 0.42025983 0.42021318 0.42028571\n",
            " 0.42027552 0.42026117 0.4202371  0.4202493  0.42023505 0.42023472\n",
            " 0.42027739 0.42029086 0.42031397 0.42031906 0.42033655 0.42032645\n",
            " 0.4203205  0.4203022  0.42030727 0.42029589 0.42030506 0.42030328\n",
            " 0.42031924 0.42029837 0.42029796 0.42032745 0.42032157 0.42031978\n",
            " 0.42030717 0.42032838 0.42031984 0.42028571 0.42031494 0.42032796\n",
            " 0.42030872 0.42031233 0.42032664 0.42031016 0.42029773 0.42026533\n",
            " 0.42026232 0.42025266 0.42024303 0.42027586 0.42028477 0.42029101\n",
            " 0.42028402 0.42029288 0.4203083  0.42032105 0.42031669 0.4203399\n",
            " 0.42034731 0.42035602 0.42035425 0.42036292 0.42035332 0.42034375\n",
            " 0.42036411 0.42037403 0.42037484 0.42037176 0.42039198 0.42036951\n",
            " 0.42038839 0.42036469 0.42041441 0.4204383  0.42042105 0.4204141\n",
            " 0.42036876 0.4203798  0.42034483 0.42030102 0.42030446 0.4203117\n",
            " 0.42031766 0.42032614 0.4203384  0.42030506 0.42029836 0.42033712\n",
            " 0.42032535 0.42030605 0.42034465 0.42031407 0.42029486 0.4202782\n",
            " 0.42028661 0.42029125 0.42030712 0.4203192  0.42031382 0.42029104\n",
            " 0.42030311 0.42028908 0.42024783 0.42024257 0.42022002 0.42022222\n",
            " 0.42021455 0.42021675 0.42023616 0.42024939 0.42022577 0.42020956\n",
            " 0.42020318 0.42021027 0.42022589 0.42023171 0.42024239 0.42021655\n",
            " 0.42020413 0.42016748 0.42021939 0.42023245 0.42024426 0.42020652\n",
            " 0.42017732 0.42017831 0.42017088 0.42013942 0.42016327 0.4201271\n",
            " 0.42012934 0.42015431 0.42014337 0.42012888 0.420118   0.42013571\n",
            " 0.42012723 0.42011401 0.42008422 0.4200628  0.42007337 0.42008511\n",
            " 0.42006612 0.42007783 0.42008127 0.4201     0.42011633 0.42010329\n",
            " 0.42008792 0.42005504 0.42004795 0.42008879 0.42010035 0.42009091\n",
            " 0.42014435 0.42013837 0.4201417  0.42015661 0.42014948 0.42014931\n",
            " 0.4201711  0.42018014 0.42019493 0.42021774 0.42017261 0.42018966\n",
            " 0.42019747 0.42020413 0.42019244 0.42015904 0.42018629 0.42016895\n",
            " 0.42014937 0.42015604 0.42018544 0.42018068 0.42018502 0.42019955\n",
            " 0.42021744 0.42019231 0.42015254 0.42017043 0.42014543 0.42017793\n",
            " 0.42016535 0.42014831 0.42012795 0.42015807 0.42016461 0.42017226\n",
            " 0.42015978 0.42017634 0.42016054 0.42020379 0.42020022 0.42020889\n",
            " 0.42022087 0.42021064 0.42019712 0.4201781  0.42017348 0.42015784\n",
            " 0.42014002 0.42012665 0.42013971 0.42013736 0.42014929 0.42014364\n",
            " 0.42011939 0.42011707 0.42014863 0.42017576 0.4201843  0.42020044\n",
            " 0.42020675 0.42023261 0.42024647 0.42026247 0.42026869 0.4202868\n",
            " 0.42029838 0.4202635  0.42025458 0.42023707 0.42024543 0.42021613\n",
            " 0.42021697 0.42020064 0.42019185 0.42016702 0.42016578 0.42019338\n",
            " 0.42016435 0.42017164 0.42019808 0.42019681 0.42018278 0.42017728\n",
            " 0.4201877  0.42021186 0.42020423 0.42019239 0.4201774  0.42018987\n",
            " 0.42015701 0.42018211 0.42017245 0.42016807 0.42019203 0.4202044\n",
            " 0.42017487 0.4201454  0.4201139  0.420119   0.4201074  0.42011875\n",
            " 0.4201384  0.42016944 0.42018795 0.42020332 0.42021969 0.42021118\n",
            " 0.42021096 0.42021281 0.42021259 0.42026598 0.4202688  0.42026955\n",
            " 0.42025797 0.42027515 0.42029026 0.42025512 0.42026203 0.42026278\n",
            " 0.42025843 0.42026531 0.42028746 0.4203055  0.42030417 0.4203252\n",
            " 0.42034822 0.42036714 0.42035461 0.4203249  0.42032457 0.42032626\n",
            " 0.42028658 0.42026411 0.42025176 0.4202505  0.4202603  0.42027912\n",
            " 0.42027884 0.42023848 0.42022523 0.420214  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "    def choose_arm(epsilon, estimates):\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(num_arms)\n",
        "        else:\n",
        "            return np.argmax(estimates)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        chosen_arm = choose_arm(epsilon, estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm:\", estimated_mean_rewards_per_arm)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7l5pyRi1XAYu",
        "outputId": "f10fde60-061d-4cbe-8f25-c3cb00d5c52d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Mean Rewards for Each Arm: [0.36113209 0.27078373 0.45047551 0.6319367  0.18084754]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 0.1\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "\n",
        "    for step in range(num_steps):\n",
        "\n",
        "        chosen_arm = np.argmax(estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (Greedy Approach):\", estimated_mean_rewards_per_arm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Wah_fcfcKgg",
        "outputId": "67f8a2cf-de7c-4230-b618-44463b87ee2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Mean Rewards for Each Arm (Greedy Approach): [0.36148753 0.         0.         0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "total_rewards_per_trial = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        # Greedy approach: Always choose the arm with the highest estimated mean reward\n",
        "        chosen_arm = np.argmax(estimates)\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "mean_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (Greedy Approach):\", estimated_mean_rewards_per_arm)\n",
        "print(\"Mean Total Reward across Trials:\", mean_total_reward)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j3NmfR3dKiR",
        "outputId": "f36ae157-20e6-48bf-a291-cf802d21e0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Mean Rewards for Each Arm (Greedy Approach): [0.36123689 0.         0.         0.         0.        ]\n",
            "Mean Total Reward across Trials: 571.519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "num_arms = 5\n",
        "true_rewards = [4, 3, 5, 7, 2]\n",
        "\n",
        "# Scale true_rewards to the [0, 1] range\n",
        "max_true_reward = max(true_rewards)\n",
        "true_rewards = [reward / max_true_reward for reward in true_rewards]\n",
        "\n",
        "num_steps = 1000\n",
        "epsilon = 3\n",
        "num_trials = 1000\n",
        "\n",
        "estimated_mean_rewards_per_arm = np.zeros(num_arms)\n",
        "total_rewards_per_trial = []\n",
        "\n",
        "for _ in range(num_trials):\n",
        "    estimates = np.zeros(num_arms)\n",
        "    arm_selection_counts = np.zeros(num_arms)\n",
        "    cumulative_reward = 0\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        if np.random.rand() < epsilon:\n",
        "\n",
        "            chosen_arm = np.random.choice(num_arms)\n",
        "        else:\n",
        "\n",
        "            chosen_arm = np.argmax(estimates)\n",
        "\n",
        "        arm_selection_counts[chosen_arm] += 1\n",
        "        reward = np.random.binomial(1, true_rewards[chosen_arm])\n",
        "        cumulative_reward += reward\n",
        "        estimates[chosen_arm] += (reward - estimates[chosen_arm]) / arm_selection_counts[chosen_arm]\n",
        "\n",
        "    total_rewards_per_trial.append(cumulative_reward)\n",
        "    estimated_mean_rewards_per_arm += (estimates - estimated_mean_rewards_per_arm) / (num_trials + 1)\n",
        "\n",
        "mean_total_reward = np.mean(total_rewards_per_trial)\n",
        "\n",
        "print(\"Estimated Mean Rewards for Each Arm (ε-Greedy Approach):\", estimated_mean_rewards_per_arm)\n",
        "print(\"Mean Total Reward across Trials:\", mean_total_reward)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWZO_j-Dd6_j",
        "outputId": "63e94985-bc8a-4f16-8447-96237b8c14a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Mean Rewards for Each Arm (ε-Greedy Approach): [0.36005142 0.27130865 0.45079742 0.6319367  0.18053297]\n",
            "Mean Total Reward across Trials: 599.952\n"
          ]
        }
      ]
    }
  ]
}